# Prometheus & Grafana 모니터링 적용 방안

## 목차

1. [개요](#1-개요)
2. [현재 시스템 분석](#2-현재-시스템-분석)
3. [적용 방식별 비교](#3-적용-방식별-비교)
4. [인프라 모니터링](#4-인프라-모니터링)
5. [Celery 배치 작업 모니터링](#5-celery-배치-작업-모니터링)
6. [배포 구성](#6-배포-구성)
7. [Grafana 대시보드 구성안](#7-grafana-대시보드-구성안)
8. [권장 방안](#8-권장-방안)

---

## 1. 개요

### 1.1 왜 Prometheus & Grafana인가

현재 Announce-Go API는 structlog 기반의 JSON 로깅만 존재하며, 시스템 상태를 실시간으로 파악하거나 이상 징후를 사전에 감지할 수 있는 메트릭 모니터링 체계가 없다.

**Prometheus**는 Pull 기반 시계열 데이터베이스로, 아래와 같은 이유로 선택한다:

- **Pull 모델**: 모니터링 대상이 `/metrics` 엔드포인트만 노출하면 Prometheus가 주기적으로 수집 (scrape)
- **다차원 데이터 모델**: 레이블 기반 쿼리로 유연한 집계 가능
- **PromQL**: 강력한 쿼리 언어로 복잡한 조건의 알림 규칙 정의
- **생태계**: PostgreSQL, Redis, Celery 등 주요 컴포넌트에 대한 Exporter가 이미 존재
- **경량**: 단일 바이너리, 로컬 스토리지 사용으로 운영 부담 최소

**Grafana**는 Prometheus의 사실상 표준 시각화 도구로:

- PromQL 네이티브 지원
- 대시보드 템플릿 공유 생태계 (Grafana Dashboards)
- 알림(Alerting) 기능 내장 (Slack, Email 등 연동)

### 1.2 기본 아키텍처

```
┌─────────────────────────────────────────────────────────┐
│                    Docker Compose                        │
│                                                          │
│  ┌──────────┐    ┌──────────┐    ┌──────────────────┐   │
│  │ FastAPI   │    │ Celery   │    │ Celery Beat      │   │
│  │ :8000     │    │ Worker   │    │ (RedBeat)        │   │
│  │ /metrics  │    └────┬─────┘    └──────────────────┘   │
│  └────┬──────┘         │                                  │
│       │           ┌────▼──────┐                           │
│       │           │ Pushgateway│  ← 배치 작업 메트릭      │
│       │           │ :9091      │                           │
│       │           └────┬──────┘                           │
│  ┌────▼────────────────▼──────────────────────────────┐  │
│  │              Prometheus :9090                       │  │
│  │  scrape: FastAPI, Pushgateway, Exporters           │  │
│  └────────────────────┬──────────────────────────────┘  │
│                       │                                   │
│  ┌────────────────────▼──────────────────────────────┐  │
│  │              Grafana :3000                          │  │
│  │  Dashboard + Alerting                               │  │
│  └────────────────────────────────────────────────────┘  │
│                                                          │
│  ┌─────────────┐ ┌──────────────┐ ┌──────────────────┐  │
│  │ postgres    │ │ redis        │ │ node             │  │
│  │ exporter    │ │ exporter     │ │ exporter         │  │
│  │ :9187       │ │ :9121        │ │ :9100            │  │
│  └─────────────┘ └──────────────┘ └──────────────────┘  │
└─────────────────────────────────────────────────────────┘
```

---

## 2. 현재 시스템 분석

### 2.1 모니터링 대상 컴포넌트

| 컴포넌트 | 역할 | 현재 상태 |
|----------|------|----------|
| **FastAPI** (uvicorn :8000) | API 서버 | `/health` 엔드포인트만 존재 (단순 `{"status": "ok"}`) |
| **Celery Worker** (concurrency=1) | 배치 크롤링 실행 | structlog 로깅만 존재, 메트릭 없음 |
| **Celery Beat** (RedBeat) | 스케줄러 (매일 01:00 KST) | 실행 여부 확인 수단 없음 |
| **PostgreSQL** | 메인 DB | pool_size=10, max_overflow=20, 모니터링 없음 |
| **Redis** | 세션 + Celery 브로커/백엔드 | 연결 상태 확인 수단 없음 |
| **Docker 컨테이너** | 단일 컨테이너 배포 | Docker healthcheck만 존재 (30s 간격) |

### 2.2 현재 로깅 체계

```
app/core/logging.py
├── Structlog + stdlib 통합
├── JSON 포맷 (stdout)
├── 프로세스 자동 식별: api / worker / beat
├── KST 타임스탬프 (ISO 포맷)
└── 필드 순서: process → timestamp → level → logger → event → [custom]
```

**로깅과 메트릭의 차이**: 로그는 개별 이벤트를 기록하지만, 메트릭은 집계된 수치를 시계열로 제공한다. "지난 5분간 API 평균 응답 시간"과 같은 질문에는 로그가 아닌 메트릭이 필요하다.

### 2.3 핵심 모니터링 지표 (수집 대상)

| 카테고리 | 지표 | 설명 |
|----------|------|------|
| **API** | 요청 지연 시간 | 엔드포인트별, HTTP 메서드별 |
| | HTTP 상태 코드 분포 | 4xx, 5xx 비율 추적 |
| | 활성 연결 수 | 동시 요청 수 |
| **DB** | 커넥션 풀 사용률 | pool_size 대비 사용 중인 연결 수 |
| | 쿼리 지연 시간 | 느린 쿼리 감지 |
| **Celery** | 태스크 실행 시간 | `crawl_all_active_trackings` 소요 시간 |
| | 성공/실패율 | 배치 작업 안정성 |
| | 큐 깊이 | 대기 중인 태스크 수 |
| **Redis** | 메모리 사용량 | OOM 사전 감지 |
| | 커맨드 지연 시간 | 성능 병목 파악 |
| **Application** | 크롤링 성공률 | 네이버 검색 순위 수집 성공/실패 |
| | 세션 수 | 활성 사용자 파악 |

---

## 3. 적용 방식별 비교

FastAPI 애플리케이션에 Prometheus 메트릭을 노출하는 3가지 방식을 비교한다.

### 3.1 방식 1: `prometheus-fastapi-instrumentator`

FastAPI 전용 자동 계측 라이브러리. 미들웨어로 등록하면 HTTP 요청 관련 메트릭이 자동 수집된다.

**설치**

```bash
pip install prometheus-fastapi-instrumentator
```

**적용 코드**

```python
# app/main.py
from prometheus_fastapi_instrumentator import Instrumentator

app = FastAPI(title="Announce-Go API", version="0.1.0", lifespan=lifespan)

# 자동 계측 활성화
Instrumentator().instrument(app).expose(app, endpoint="/metrics")
```

**자동 수집되는 메트릭**

| 메트릭 | 타입 | 설명 |
|--------|------|------|
| `http_requests_total` | Counter | 총 HTTP 요청 수 (method, status, handler 레이블) |
| `http_request_duration_seconds` | Histogram | 요청 처리 시간 분포 |
| `http_request_size_bytes` | Summary | 요청 본문 크기 |
| `http_response_size_bytes` | Summary | 응답 본문 크기 |
| `http_requests_in_progress` | Gauge | 현재 처리 중인 요청 수 |

**커스텀 메트릭 추가**

```python
from prometheus_fastapi_instrumentator.metrics import Info
from prometheus_client import Counter

# 크롤링 결과 카운터 (직접 정의)
CRAWL_SUCCESS = Counter("crawl_success_total", "Successful crawls", ["rank_type"])
CRAWL_FAILURE = Counter("crawl_failure_total", "Failed crawls", ["rank_type"])

# Instrumentator 콜백으로 추가 메트릭 연결
def crawl_metrics(info: Info):
    # 요청 기반 커스텀 로직
    pass

Instrumentator().add(crawl_metrics).instrument(app).expose(app)
```

**장점**
- 코드 2~3줄로 즉시 적용 가능
- HTTP 요청 관련 기본 메트릭이 포괄적으로 자동 수집
- FastAPI 라우터/엔드포인트 이름이 레이블에 자동 매핑
- `/metrics` 엔드포인트 자동 생성

**단점**
- FastAPI HTTP 요청 메트릭에 한정 (Celery, DB 풀 등은 별도 처리 필요)
- `prometheus_client` 의존 (내부적으로 사용)
- 커스텀 메트릭 추가 시 결국 `prometheus_client` 직접 사용 필요
- FastAPI 전용이므로 프레임워크 변경 시 마이그레이션 필요

---

### 3.2 방식 2: `prometheus_client` 직접 사용

Prometheus 공식 Python 클라이언트로 메트릭을 수동 정의하고 수집한다.

**설치**

```bash
pip install prometheus-client
```

**적용 코드**

```python
# app/core/metrics.py
from prometheus_client import Counter, Histogram, Gauge, generate_latest, CONTENT_TYPE_LATEST

# HTTP 메트릭 (수동 정의)
REQUEST_COUNT = Counter(
    "http_requests_total",
    "Total HTTP requests",
    ["method", "endpoint", "status_code"],
)
REQUEST_LATENCY = Histogram(
    "http_request_duration_seconds",
    "HTTP request latency",
    ["method", "endpoint"],
    buckets=[0.01, 0.05, 0.1, 0.25, 0.5, 1.0, 2.5, 5.0],
)

# 비즈니스 메트릭
CRAWL_DURATION = Histogram(
    "crawl_task_duration_seconds",
    "Crawl task execution duration",
    ["rank_type"],
)
CRAWL_RESULT = Counter(
    "crawl_result_total",
    "Crawl task results",
    ["rank_type", "status"],  # status: success / failure
)
ACTIVE_TRACKINGS = Gauge(
    "active_trackings_count",
    "Number of active rank trackings",
    ["rank_type"],
)

# DB 커넥션 풀 메트릭
DB_POOL_SIZE = Gauge("db_pool_checked_out", "DB connections currently in use")
DB_POOL_OVERFLOW = Gauge("db_pool_overflow", "DB connections in overflow")
```

```python
# app/main.py - 미들웨어로 HTTP 메트릭 수집
import time
from starlette.middleware.base import BaseHTTPMiddleware
from starlette.requests import Request
from starlette.responses import Response
from app.core.metrics import REQUEST_COUNT, REQUEST_LATENCY, generate_latest, CONTENT_TYPE_LATEST

class PrometheusMiddleware(BaseHTTPMiddleware):
    async def dispatch(self, request: Request, call_next):
        start = time.perf_counter()
        response = await call_next(request)
        duration = time.perf_counter() - start

        endpoint = request.url.path
        method = request.method
        status = str(response.status_code)

        REQUEST_COUNT.labels(method=method, endpoint=endpoint, status_code=status).inc()
        REQUEST_LATENCY.labels(method=method, endpoint=endpoint).observe(duration)
        return response

app.add_middleware(PrometheusMiddleware)

@app.get("/metrics")
async def metrics():
    return Response(generate_latest(), media_type=CONTENT_TYPE_LATEST)
```

```python
# app/tasks/rank_tasks.py - Celery 태스크에서 메트릭 기록
from app.core.metrics import CRAWL_DURATION, CRAWL_RESULT

@celery_app.task
def crawl_all_active_trackings():
    for tracking in trackings:
        start = time.perf_counter()
        try:
            result = crawl(tracking)
            CRAWL_RESULT.labels(rank_type=tracking.type, status="success").inc()
        except Exception:
            CRAWL_RESULT.labels(rank_type=tracking.type, status="failure").inc()
        finally:
            CRAWL_DURATION.labels(rank_type=tracking.type).observe(time.perf_counter() - start)
```

**장점**
- 완전한 제어권: 메트릭 이름, 레이블, 버킷 등을 자유롭게 설계
- HTTP뿐 아니라 Celery 태스크, DB 풀, 비즈니스 로직 등 모든 영역에 적용 가능
- 외부 의존성이 `prometheus_client` 하나뿐
- Prometheus 메트릭 타입(Counter, Gauge, Histogram, Summary) 직접 선택 가능

**단점**
- 모든 메트릭을 수동 정의해야 하므로 초기 작업량이 많음
- HTTP 요청 미들웨어를 직접 구현해야 함
- 메트릭 누락 위험 (수동 관리이므로 빠뜨릴 수 있음)
- Celery Worker 프로세스와 FastAPI 프로세스 간 메트릭 공유 문제 (multiprocess mode 또는 Pushgateway 필요)

---

### 3.3 방식 3: OpenTelemetry + Prometheus Exporter

벤더 중립 표준인 OpenTelemetry(OTel)로 계측하고, Prometheus 형식으로 내보낸다.

**설치**

```bash
pip install opentelemetry-api \
    opentelemetry-sdk \
    opentelemetry-instrumentation-fastapi \
    opentelemetry-instrumentation-sqlalchemy \
    opentelemetry-instrumentation-redis \
    opentelemetry-instrumentation-celery \
    opentelemetry-exporter-prometheus
```

**적용 코드**

```python
# app/core/telemetry.py
from opentelemetry import metrics
from opentelemetry.sdk.metrics import MeterProvider
from opentelemetry.exporter.prometheus import PrometheusMetricReader
from opentelemetry.instrumentation.fastapi import FastAPIInstrumentor
from opentelemetry.instrumentation.sqlalchemy import SQLAlchemyInstrumentor
from opentelemetry.instrumentation.redis import RedisInstrumentor
from opentelemetry.instrumentation.celery import CeleryInstrumentor

def setup_telemetry(app, engine):
    # Prometheus exporter 설정
    reader = PrometheusMetricReader()
    provider = MeterProvider(metric_readers=[reader])
    metrics.set_meter_provider(provider)

    # 자동 계측 활성화
    FastAPIInstrumentor.instrument_app(app)
    SQLAlchemyInstrumentor().instrument(engine=engine)
    RedisInstrumentor().instrument()
    CeleryInstrumentor().instrument()

    # 커스텀 메트릭
    meter = metrics.get_meter("announce-go")
    crawl_counter = meter.create_counter(
        "crawl_result_total",
        description="Crawl task results",
    )
    return meter
```

```python
# app/main.py
from app.core.telemetry import setup_telemetry
from prometheus_client import start_http_server

@asynccontextmanager
async def lifespan(app: FastAPI):
    # ... 기존 초기화 ...
    setup_telemetry(app, engine)
    yield
    # ... 기존 종료 ...
```

**장점**
- **벤더 중립**: Prometheus 외에도 Jaeger, Datadog, New Relic 등으로 전환/병행 가능
- **자동 계측**: FastAPI, SQLAlchemy, Redis, Celery 계측 라이브러리가 모두 존재
- **분산 트레이싱**: 메트릭뿐 아니라 요청 추적(Tracing)도 동시에 적용 가능
- **업계 표준**: CNCF 프로젝트로 장기적 지원 보장

**단점**
- **의존성 과다**: 패키지 8~10개 이상 추가 필요
- **학습 곡선**: OTel SDK 개념 (Tracer, Meter, Span, Resource 등) 이해 필요
- **오버헤드**: 계측 레이어가 두꺼워 약간의 성능 영향
- **복잡도**: 현재 규모(단일 서비스)에서는 과도한 인프라
- **Prometheus 변환 손실**: OTel 메트릭 → Prometheus 변환 시 일부 기능 제약

---

### 3.4 방식별 비교 요약

| 비교 항목 | 방식 1: instrumentator | 방식 2: prometheus_client | 방식 3: OpenTelemetry |
|-----------|----------------------|-------------------------|---------------------|
| **초기 적용 난이도** | 매우 쉬움 (2~3줄) | 보통 (미들웨어+메트릭 직접 정의) | 어려움 (SDK 설정+패키지 다수) |
| **HTTP 메트릭** | 자동 | 수동 (미들웨어 구현) | 자동 |
| **Celery 메트릭** | 별도 처리 필요 | 수동 가능 | 자동 (instrumentation) |
| **DB/Redis 메트릭** | 별도 처리 필요 | 수동 가능 | 자동 (instrumentation) |
| **커스텀 메트릭** | `prometheus_client` 혼용 | 자유로움 | OTel Meter API 사용 |
| **추가 패키지 수** | 1개 | 1개 | 8~10개 |
| **벤더 종속성** | Prometheus 전용 | Prometheus 전용 | 벤더 중립 |
| **분산 트레이싱** | 불가 | 불가 | 가능 |
| **운영 복잡도** | 낮음 | 낮음 | 높음 |
| **적합한 규모** | 소~중규모 단일 서비스 | 모든 규모 | 중~대규모 MSA |

---

## 4. 인프라 모니터링

애플리케이션 메트릭 외에, 인프라 컴포넌트를 모니터링하기 위한 별도 Exporter들이 필요하다.

### 4.1 PostgreSQL Exporter

| 항목 | 내용 |
|------|------|
| **이미지** | `prometheuscommunity/postgres-exporter` |
| **포트** | 9187 |
| **주요 메트릭** | `pg_stat_activity_count`, `pg_stat_database_tup_*`, `pg_stat_bgwriter_*` |
| **용도** | 활성 연결 수, 트랜잭션 처리량, 디스크 I/O, 슬로우 쿼리 감지 |

```yaml
postgres-exporter:
  image: prometheuscommunity/postgres-exporter
  environment:
    DATA_SOURCE_NAME: "postgresql://user:password@postgres:5432/announce_go?sslmode=disable"
  ports:
    - "9187:9187"
```

**핵심 모니터링 지표**
- `pg_stat_activity_count` — 활성 연결 수 (pool_size=10 대비 모니터링)
- `pg_stat_database_xact_commit` / `xact_rollback` — 커밋/롤백 비율
- `pg_stat_database_blks_hit` / `blks_read` — 캐시 히트율
- `pg_stat_user_tables_n_tup_ins/upd/del` — 테이블별 DML 처리량

### 4.2 Redis Exporter

| 항목 | 내용 |
|------|------|
| **이미지** | `oliver006/redis_exporter` |
| **포트** | 9121 |
| **주요 메트릭** | `redis_memory_used_bytes`, `redis_connected_clients`, `redis_commands_total` |
| **용도** | 메모리 사용량, 연결 수, 명령 처리량, 키 만료 상태 |

```yaml
redis-exporter:
  image: oliver006/redis_exporter
  environment:
    REDIS_ADDR: "redis://redis:6379"
    REDIS_PASSWORD: "${REDIS_PASSWORD}"
  ports:
    - "9121:9121"
```

**핵심 모니터링 지표**
- `redis_memory_used_bytes` — OOM 사전 감지 (세션 + Celery 브로커 공유)
- `redis_connected_clients` — 클라이언트 연결 수
- `redis_keyspace_hits` / `keyspace_misses` — 캐시 효율
- `redis_commands_processed_total` — 초당 명령 처리량

### 4.3 Celery Exporter (+ Flower)

#### 방법 A: Celery Exporter

| 항목 | 내용 |
|------|------|
| **이미지** | `danihodovic/celery-exporter` |
| **포트** | 9808 |
| **주요 메트릭** | `celery_task_*`, `celery_worker_*`, `celery_queue_length` |
| **용도** | 태스크 실행/성공/실패 횟수, 큐 깊이, 워커 상태 |

```yaml
celery-exporter:
  image: danihodovic/celery-exporter
  environment:
    CE_BROKER_URL: "redis://redis:6379/0"
  ports:
    - "9808:9808"
```

#### 방법 B: Flower (모니터링 + 웹 UI)

| 항목 | 내용 |
|------|------|
| **이미지** | `mher/flower` |
| **포트** | 5555 (UI), /metrics (Prometheus) |
| **추가 기능** | 웹 대시보드, 태스크 재시도, 워커 관리 |

```yaml
flower:
  image: mher/flower
  command: celery --broker=redis://redis:6379/0 flower --port=5555
  environment:
    CELERY_BROKER_URL: "redis://redis:6379/0"
  ports:
    - "5555:5555"
```

**Celery Exporter vs Flower**

| 비교 항목 | Celery Exporter | Flower |
|-----------|----------------|--------|
| 메트릭 전용 | O | O (+ 웹 UI) |
| 리소스 사용 | 경량 | 상대적으로 무거움 |
| 태스크 관리 기능 | X | O (재시도, 취소 등) |
| 프로덕션 권장 | O | 개발/스테이징 용도 |

### 4.4 cAdvisor (Docker 컨테이너)

| 항목 | 내용 |
|------|------|
| **이미지** | `gcr.io/cadvisor/cadvisor` |
| **포트** | 8080 |
| **주요 메트릭** | `container_cpu_usage_seconds_total`, `container_memory_usage_bytes`, `container_network_*` |
| **용도** | 컨테이너별 CPU/메모리/네트워크/디스크 사용량 |

```yaml
cadvisor:
  image: gcr.io/cadvisor/cadvisor:latest
  volumes:
    - /:/rootfs:ro
    - /var/run:/var/run:ro
    - /sys:/sys:ro
    - /var/lib/docker/:/var/lib/docker:ro
  ports:
    - "8080:8080"
```

**핵심 모니터링 지표**
- `container_cpu_usage_seconds_total` — 컨테이너 CPU 사용률
- `container_memory_usage_bytes` — 메모리 사용량 (Playwright 크롤러 메모리 누수 감지)
- `container_network_receive_bytes_total` — 네트워크 I/O
- `container_fs_usage_bytes` — 디스크 사용량

### 4.5 Node Exporter (호스트 머신)

| 항목 | 내용 |
|------|------|
| **이미지** | `prom/node-exporter` |
| **포트** | 9100 |
| **주요 메트릭** | `node_cpu_seconds_total`, `node_memory_*`, `node_disk_*`, `node_filesystem_*` |
| **용도** | 호스트 OS 레벨 리소스 모니터링 |

```yaml
node-exporter:
  image: prom/node-exporter
  volumes:
    - /proc:/host/proc:ro
    - /sys:/host/sys:ro
    - /:/rootfs:ro
  command:
    - "--path.procfs=/host/proc"
    - "--path.sysfs=/host/sys"
    - "--path.rootfs=/rootfs"
  ports:
    - "9100:9100"
```

### 4.6 Exporter 요약

| Exporter | 포트 | 대상 | 필수도 |
|----------|------|------|--------|
| PostgreSQL Exporter | 9187 | DB 상태 | 높음 |
| Redis Exporter | 9121 | 세션/브로커 상태 | 높음 |
| Celery Exporter | 9808 | 배치 작업 | 높음 |
| cAdvisor | 8080 | 컨테이너 리소스 | 중간 |
| Node Exporter | 9100 | 호스트 머신 | 중간 (클라우드 시 선택) |

---

## 5. Celery 배치 작업 모니터링

Celery Worker는 별도 프로세스로 실행되므로, FastAPI의 `/metrics` 엔드포인트와는 다른 방식으로 메트릭을 수집해야 한다.

### 5.1 Pushgateway 방식

배치 작업이 완료 시점에 결과 메트릭을 Pushgateway에 Push하고, Prometheus가 Pushgateway를 Scrape한다.

```
Celery Worker → (push) → Pushgateway :9091 → (scrape) ← Prometheus
```

**적용 코드**

```python
# app/tasks/rank_tasks.py
from prometheus_client import CollectorRegistry, Counter, Gauge, Histogram, push_to_gateway

PUSHGATEWAY_URL = "pushgateway:9091"

@celery_app.task
def crawl_all_active_trackings():
    registry = CollectorRegistry()

    duration = Gauge("batch_crawl_duration_seconds", "Batch crawl duration", registry=registry)
    total = Gauge("batch_crawl_total", "Total trackings processed", registry=registry)
    success = Gauge("batch_crawl_success", "Successful crawls", registry=registry)
    failure = Gauge("batch_crawl_failure", "Failed crawls", registry=registry)

    start = time.time()
    # ... 크롤링 로직 ...
    elapsed = time.time() - start

    duration.set(elapsed)
    total.set(result["total"])
    success.set(result["success"])
    failure.set(result["fail"])

    push_to_gateway(PUSHGATEWAY_URL, job="crawl_batch", registry=registry)
```

**장점**
- 배치 작업 특성에 적합 (실행 후 종료되는 short-lived job)
- 작업 완료 시점의 스냅샷 메트릭을 정확히 기록
- Worker 프로세스가 HTTP 서버를 열 필요 없음

**단점**
- Pushgateway는 단일 장애점(SPOF)이 될 수 있음
- 메트릭 업데이트 시점이 push 순간에 한정 (실시간성 부족)
- Pushgateway 장애 시 메트릭 유실
- "마지막 push" 값이 유지되므로 stale 메트릭 관리 필요

### 5.2 Celery Exporter 방식

별도 프로세스(Celery Exporter)가 Celery 이벤트를 구독하여 메트릭을 수집한다.

```
Celery Worker → (events) → Redis Broker → (subscribe) ← Celery Exporter :9808 → (scrape) ← Prometheus
```

**적용**: 별도 코드 변경 없이 Docker 컨테이너만 추가하면 된다.

**장점**
- 애플리케이션 코드 수정 불필요
- 태스크 상태(PENDING, STARTED, SUCCESS, FAILURE) 자동 추적
- 실시간 이벤트 기반 메트릭 수집

**단점**
- 비즈니스 메트릭(크롤링 성공률 등)은 수집 불가 (태스크 메타데이터만)
- Celery 이벤트 활성화 필요 (`worker_send_task_events = True`)
- 커스텀 메트릭 정의 불가

### 5.3 비교 및 권장

| 비교 항목 | Pushgateway | Celery Exporter |
|-----------|-------------|-----------------|
| 코드 수정 | 필요 (push 로직) | 불필요 |
| 비즈니스 메트릭 | 가능 (자유 정의) | 불가 |
| 실시간성 | 작업 완료 시점만 | 실시간 이벤트 |
| 인프라 추가 | Pushgateway 컨테이너 | Celery Exporter 컨테이너 |
| 적합한 경우 | 배치 결과 상세 모니터링 | 태스크 흐름 기본 모니터링 |

**권장**: 두 방식을 **병행** 사용한다.
- **Celery Exporter**: 태스크 상태 흐름, 큐 깊이 등 기본 모니터링
- **Pushgateway**: 크롤링 성공/실패 수, 소요 시간 등 비즈니스 메트릭

---

## 6. 배포 구성

### 6.1 Docker Compose 확장

기존 `docker-compose.yml`에 모니터링 스택을 추가하는 예시이다.

```yaml
# docker-compose.monitoring.yml
# 사용: docker compose -f docker-compose.yml -f docker-compose.monitoring.yml up -d

services:
  # ============================================
  # Prometheus
  # ============================================
  prometheus:
    image: prom/prometheus:latest
    volumes:
      - ./monitoring/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
      - ./monitoring/prometheus/alert_rules.yml:/etc/prometheus/alert_rules.yml
      - prometheus_data:/prometheus
    ports:
      - "9090:9090"
    command:
      - "--config.file=/etc/prometheus/prometheus.yml"
      - "--storage.tsdb.retention.time=30d"
    restart: unless-stopped

  # ============================================
  # Grafana
  # ============================================
  grafana:
    image: grafana/grafana:latest
    volumes:
      - ./monitoring/grafana/provisioning:/etc/grafana/provisioning
      - grafana_data:/var/lib/grafana
    environment:
      GF_SECURITY_ADMIN_USER: admin
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_PASSWORD:-admin}
      GF_USERS_ALLOW_SIGN_UP: "false"
    ports:
      - "3000:3000"
    depends_on:
      - prometheus
    restart: unless-stopped

  # ============================================
  # Pushgateway (Celery 배치 메트릭용)
  # ============================================
  pushgateway:
    image: prom/pushgateway:latest
    ports:
      - "9091:9091"
    restart: unless-stopped

  # ============================================
  # Exporters
  # ============================================
  postgres-exporter:
    image: prometheuscommunity/postgres-exporter
    environment:
      DATA_SOURCE_NAME: "postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB}?sslmode=disable"
    ports:
      - "9187:9187"
    depends_on:
      - postgres
    restart: unless-stopped

  redis-exporter:
    image: oliver006/redis_exporter
    environment:
      REDIS_ADDR: "redis://redis:6379"
      REDIS_PASSWORD: "${REDIS_PASSWORD}"
    ports:
      - "9121:9121"
    depends_on:
      - redis
    restart: unless-stopped

  celery-exporter:
    image: danihodovic/celery-exporter
    environment:
      CE_BROKER_URL: "redis://:${REDIS_PASSWORD}@redis:6379/0"
    ports:
      - "9808:9808"
    depends_on:
      - redis
    restart: unless-stopped

  cadvisor:
    image: gcr.io/cadvisor/cadvisor:latest
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:ro
      - /sys:/sys:ro
      - /var/lib/docker/:/var/lib/docker:ro
    ports:
      - "8080:8080"
    restart: unless-stopped

  node-exporter:
    image: prom/node-exporter
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    command:
      - "--path.procfs=/host/proc"
      - "--path.sysfs=/host/sys"
      - "--path.rootfs=/rootfs"
    ports:
      - "9100:9100"
    restart: unless-stopped

volumes:
  prometheus_data:
  grafana_data:
```

### 6.2 Prometheus 설정 파일

```yaml
# monitoring/prometheus/prometheus.yml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

rule_files:
  - "alert_rules.yml"

scrape_configs:
  # FastAPI 애플리케이션
  - job_name: "fastapi"
    static_configs:
      - targets: ["api:8000"]
    metrics_path: "/metrics"

  # Pushgateway (Celery 배치 메트릭)
  - job_name: "pushgateway"
    honor_labels: true
    static_configs:
      - targets: ["pushgateway:9091"]

  # PostgreSQL
  - job_name: "postgresql"
    static_configs:
      - targets: ["postgres-exporter:9187"]

  # Redis
  - job_name: "redis"
    static_configs:
      - targets: ["redis-exporter:9121"]

  # Celery
  - job_name: "celery"
    static_configs:
      - targets: ["celery-exporter:9808"]

  # Docker 컨테이너
  - job_name: "cadvisor"
    static_configs:
      - targets: ["cadvisor:8080"]

  # 호스트 머신
  - job_name: "node"
    static_configs:
      - targets: ["node-exporter:9100"]
```

### 6.3 알림 규칙 예시

```yaml
# monitoring/prometheus/alert_rules.yml
groups:
  - name: announce-go-alerts
    rules:
      # API 서버 다운
      - alert: APIServerDown
        expr: up{job="fastapi"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "FastAPI 서버가 응답하지 않습니다"

      # API 응답 시간 초과
      - alert: HighRequestLatency
        expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket{job="fastapi"}[5m])) > 2
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "API p95 응답 시간이 2초를 초과했습니다"

      # 5xx 에러율
      - alert: HighErrorRate
        expr: rate(http_requests_total{status_code=~"5.."}[5m]) / rate(http_requests_total[5m]) > 0.05
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "5xx 에러율이 5%를 초과했습니다"

      # 배치 크롤링 실패
      - alert: CrawlBatchFailure
        expr: batch_crawl_failure > 0
        for: 0m
        labels:
          severity: warning
        annotations:
          summary: "배치 크롤링에서 {{ $value }}건의 실패가 발생했습니다"

      # DB 커넥션 풀 고갈
      - alert: DBConnectionPoolExhausted
        expr: pg_stat_activity_count > 8
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "PostgreSQL 활성 연결이 {{ $value }}개입니다 (pool_size=10)"

      # Redis 메모리 사용량
      - alert: RedisHighMemory
        expr: redis_memory_used_bytes / redis_memory_max_bytes > 0.8
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Redis 메모리 사용률이 80%를 초과했습니다"

      # 컨테이너 메모리 사용량 (Playwright 누수 감지)
      - alert: ContainerHighMemory
        expr: container_memory_usage_bytes{name="announce-go-api"} > 1073741824
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "API 컨테이너 메모리가 1GB를 초과했습니다 (Playwright 메모리 누수 의심)"
```

### 6.4 디렉토리 구조

```
monitoring/
├── prometheus/
│   ├── prometheus.yml          # Prometheus 설정
│   └── alert_rules.yml         # 알림 규칙
├── grafana/
│   └── provisioning/
│       ├── datasources/
│       │   └── prometheus.yml  # Prometheus 데이터소스 자동 등록
│       └── dashboards/
│           ├── dashboard.yml   # 대시보드 프로비저닝 설정
│           └── json/           # 대시보드 JSON 파일
└── docker-compose.monitoring.yml
```

---

## 7. Grafana 대시보드 구성안

### 7.1 대시보드 목록

| 대시보드 | 주요 패널 | 데이터 소스 |
|----------|----------|-------------|
| **API Overview** | RPS, 응답 시간 p50/p95/p99, 에러율, 상태코드 분포 | FastAPI /metrics |
| **Celery Tasks** | 태스크 실행 수, 성공/실패율, 큐 깊이, 워커 상태 | Celery Exporter |
| **Batch Crawling** | 일별 크롤링 결과, 소요 시간 추이, 실패 항목 | Pushgateway |
| **PostgreSQL** | 활성 연결, TPS, 캐시 히트율, 테이블 크기 | PostgreSQL Exporter |
| **Redis** | 메모리 사용량, 연결 수, 명령 처리량, 키 수 | Redis Exporter |
| **Infrastructure** | 컨테이너 CPU/메모리, 호스트 디스크/네트워크 | cAdvisor + Node Exporter |

### 7.2 API Overview 대시보드 상세

```
┌─────────────────────────────────────────────────────────────┐
│                    API Overview Dashboard                     │
├──────────────┬──────────────┬──────────────┬────────────────┤
│  Total RPS   │   Error Rate │  p95 Latency │ Active Conns   │
│   (Stat)     │   (Stat)     │   (Stat)     │   (Stat)       │
├──────────────┴──────────────┴──────────────┴────────────────┤
│           Request Rate by Endpoint (Time Series)             │
│  ▓▓▓▓░░░░░░ /api/v1/admin/trackings                        │
│  ▓▓░░░░░░░░ /api/v1/agency/work-records                    │
│  ▓░░░░░░░░░ /api/v1/common/auth/login                      │
├─────────────────────────────┬───────────────────────────────┤
│  Response Time Distribution │  Status Code Distribution     │
│       (Heatmap)             │       (Pie Chart)             │
│                             │  ■ 2xx (95%)                  │
│                             │  ■ 4xx (4%)                   │
│                             │  ■ 5xx (1%)                   │
├─────────────────────────────┴───────────────────────────────┤
│             Slowest Endpoints - Top 10 (Table)               │
│  Endpoint                    │ p50    │ p95    │ p99         │
│  /api/v1/admin/trackings     │ 120ms  │ 450ms  │ 1.2s       │
└─────────────────────────────────────────────────────────────┘
```

**PromQL 예시**

```promql
# 초당 요청 수 (RPS)
rate(http_requests_total{job="fastapi"}[5m])

# p95 응답 시간
histogram_quantile(0.95, rate(http_request_duration_seconds_bucket{job="fastapi"}[5m]))

# 에러율
sum(rate(http_requests_total{status_code=~"5.."}[5m])) / sum(rate(http_requests_total[5m]))

# 엔드포인트별 요청 수
sum by (handler) (rate(http_requests_total{job="fastapi"}[5m]))
```

### 7.3 Batch Crawling 대시보드 상세

```
┌─────────────────────────────────────────────────────────────┐
│                  Batch Crawling Dashboard                     │
├──────────────┬──────────────┬──────────────┬────────────────┤
│ Last Run     │ Success Rate │ Total Time   │ Failed Count   │
│ (Stat)       │ (Stat/Gauge) │ (Stat)       │ (Stat)         │
├──────────────┴──────────────┴──────────────┴────────────────┤
│        Daily Crawl Results (Stacked Bar - 30 days)           │
│  ■ Success  ■ Failure                                        │
│  ████████░░ ████████░░ ████████░░ ████████░░                │
├─────────────────────────────┬───────────────────────────────┤
│  Crawl Duration Trend       │  Success Rate by Type          │
│  (Time Series - 30 days)    │  (Time Series)                │
│                             │  — PLACE: 98%                  │
│                             │  — BLOG: 95%                   │
│                             │  — CAFE: 96%                   │
└─────────────────────────────┴───────────────────────────────┘
```

### 7.4 공개 대시보드 템플릿 활용

Grafana Dashboard 마켓플레이스에서 가져올 수 있는 검증된 대시보드:

| 대시보드 ID | 이름 | 용도 |
|-------------|------|------|
| 1860 | Node Exporter Full | 호스트 머신 모니터링 |
| 763 | Redis Dashboard | Redis 상세 모니터링 |
| 9628 | PostgreSQL Database | PostgreSQL 상세 모니터링 |
| 14282 | cAdvisor | Docker 컨테이너 모니터링 |

> Grafana에서 `Import Dashboard` → ID 입력으로 즉시 사용 가능

---

## 8. 권장 방안

### 8.1 프로젝트 상황 분석

| 요소 | 현재 상태 |
|------|----------|
| 서비스 규모 | 단일 서비스 (API + Worker + Beat 단일 컨테이너) |
| 팀 규모 | 소규모 (인프라 전담 인력 부재 추정) |
| 배포 환경 | Docker Compose 기반 |
| 핵심 워크로드 | 매일 01:00 배치 크롤링 + REST API |
| 기존 인프라 | PostgreSQL, Redis 이미 사용 중 |

### 8.2 단계별 적용 로드맵

#### Phase 1: 빠른 시작 (1~2일)

**목표**: 최소한의 코드 변경으로 API 메트릭 수집 시작

1. `prometheus-fastapi-instrumentator` 적용 (방식 1)
   - `pip install prometheus-fastapi-instrumentator`
   - `app/main.py`에 2줄 추가
   - HTTP 요청 메트릭 자동 수집 시작

2. Prometheus + Grafana Docker Compose 추가
   - `docker-compose.monitoring.yml` 작성
   - 기본 scrape 설정 (FastAPI만)

3. Grafana 기본 대시보드 구성
   - API Overview 대시보드 생성

#### Phase 2: 인프라 가시성 확보 (1~2일)

**목표**: DB, Redis, 컨테이너 레벨 모니터링

1. PostgreSQL Exporter 추가
2. Redis Exporter 추가
3. cAdvisor 추가
4. 각 Exporter에 대한 Grafana 대시보드 (공개 템플릿 활용)

#### Phase 3: 배치 작업 모니터링 (1~2일)

**목표**: 핵심 비즈니스인 크롤링 배치 작업의 가시성 확보

1. Celery Exporter 추가 (태스크 기본 모니터링)
2. Pushgateway 추가 + 배치 태스크에 push 로직 삽입
   - 크롤링 성공/실패 수, 소요 시간, 타입별 통계
3. Batch Crawling 대시보드 구성

#### Phase 4: 알림 체계 구축 (1일)

**목표**: 이상 징후 자동 알림

1. Prometheus alert_rules.yml 작성
2. Grafana 알림 채널 설정 (Slack, Email 등)
3. 핵심 알림 규칙 정의:
   - API 서버 다운
   - 배치 크롤링 실패
   - DB 커넥션 풀 고갈
   - 컨테이너 메모리 이상

### 8.3 최종 권장 구성

```
[권장 조합]

FastAPI 메트릭     → prometheus-fastapi-instrumentator (방식 1)
                     + prometheus_client로 커스텀 메트릭 추가 (방식 2 부분 혼용)

Celery 메트릭      → Celery Exporter (기본) + Pushgateway (비즈니스 메트릭)

인프라 메트릭      → PostgreSQL Exporter + Redis Exporter + cAdvisor

시각화 & 알림     → Grafana (대시보드 + Alerting)
```

**이유**:
- **방식 1 + 방식 2 혼합**: HTTP 메트릭은 자동 수집하되, 비즈니스 메트릭은 `prometheus_client`로 직접 정의. 가장 실용적인 조합
- **OpenTelemetry(방식 3) 미채택**: 현재 단일 서비스 규모에서 OTel의 복잡도는 과도. 향후 MSA 전환 시 재검토
- **Celery Exporter + Pushgateway 병행**: 태스크 흐름은 Exporter로, 크롤링 결과 상세는 Pushgateway로 역할 분담
- **Node Exporter 선택적**: 클라우드 환경이면 클라우드 자체 모니터링으로 대체 가능

### 8.4 예상 리소스

| 컴포넌트 | 메모리 | 디스크 (30일 보존) |
|----------|--------|-------------------|
| Prometheus | ~256MB | ~1~2GB |
| Grafana | ~128MB | ~100MB |
| Pushgateway | ~32MB | - |
| PostgreSQL Exporter | ~32MB | - |
| Redis Exporter | ~16MB | - |
| Celery Exporter | ~64MB | - |
| cAdvisor | ~128MB | - |
| **합계** | **~656MB** | **~2GB** |

> 현재 단일 서버 환경에서 약 700MB의 추가 메모리가 필요하다. 서버 스펙에 따라 cAdvisor나 Node Exporter는 생략 가능하다.
